from fastapi import FastAPI, HTTPException, Query, Body, Depends, status, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel, Field, HttpUrl, validator
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
import json
import uuid
import logging
from collections import defaultdict
import hashlib
import secrets
import jwt
import os
import asyncio
import re
from pathlib import Path

from sqlalchemy import create_engine, Column, String, Integer, Text, DateTime, ForeignKey, Table, ARRAY, func
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, Session, relationship
from sqlalchemy.dialects.postgresql import UUID, JSONB

from llm import TextClusterAnalyzer


logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ==================== DATABASE SETUP ====================
DATABASE_URL = os.getenv("DATABASE_URL", "postgresql://news_user:secure_password_123@postgres:5432/news_db")
SECRET_KEY = os.getenv("SECRET_KEY", secrets.token_urlsafe(32))
RSS_JSON_PATH = Path(os.getenv("RSS_JSON_PATH", "/app/rss_parser/output/rss_feed_4h.json"))

ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 30

engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

# ==================== DATABASE MODELS ====================
article_relations = Table(
    'article_relations',
    Base.metadata,
    Column('article_id', UUID(as_uuid=True), ForeignKey('articles.id'), primary_key=True),
    Column('related_article_id', UUID(as_uuid=True), ForeignKey('articles.id'), primary_key=True)
)

user_read_articles = Table(
    'user_read_articles',
    Base.metadata,
    Column('user_id', UUID(as_uuid=True), ForeignKey('users.id'), primary_key=True),
    Column('article_id', UUID(as_uuid=True), ForeignKey('articles.id'), primary_key=True),
    Column('read_at', DateTime, default=datetime.utcnow)
)

class User(Base):
    __tablename__ = "users"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    login = Column(String(100), unique=True, nullable=False, index=True)
    password_hash = Column(String(256), nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    interested_hashtags = Column(JSONB, default={}) 
    interested_clusters = Column(JSONB, default={})
    interested_authors = Column(JSONB, default={})
    
    read_articles = relationship("Article", secondary=user_read_articles, back_populates="read_by_users")

class Cluster(Base):
    __tablename__ = "clusters"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    cluster_number = Column(Integer, nullable=False)
    name = Column(String(500), nullable=False)
    summary = Column(Text, nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow)

    articles = relationship("Article", back_populates="cluster")

class Article(Base):
    __tablename__ = "articles"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    link = Column(String(1000), unique=True, nullable=False, index=True)
    title = Column(String(1000), nullable=False)
    summary = Column(Text)
    short_summary = Column(Text)
    source = Column(String(200))
    published = Column(String(100))
    author = Column(String(200), nullable=True)

    hashtags = Column(ARRAY(String), default=[])
    entities = Column(JSONB, default={})

    cluster_id = Column(UUID(as_uuid=True), ForeignKey('clusters.id'), nullable=True)
    cluster = relationship("Cluster", back_populates="articles")

    related_articles = relationship(
        "Article",
        secondary=article_relations,
        primaryjoin=id == article_relations.c.article_id,
        secondaryjoin=id == article_relations.c.related_article_id,
        backref="related_to"
    )
    
    read_by_users = relationship("User", secondary=user_read_articles, back_populates="read_articles")
    
    created_at = Column(DateTime, default=datetime.utcnow)

Base.metadata.create_all(bind=engine)

# ==================== PYDANTIC MODELS ====================

class UserCreate(BaseModel):
    login: str = Field(..., min_length=3, max_length=100)
    password: str = Field(..., min_length=6, max_length=100)

class UserLogin(BaseModel):
    login: str
    password: str

class Token(BaseModel):
    access_token: str
    token_type: str = "bearer"

class ArticleCreate(BaseModel):
    title: str = Field(..., description="–ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–∞—Ç—å–∏")
    summary: Optional[str] = Field(None, description="–ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ")
    link: str = Field(..., description="–°—Å—ã–ª–∫–∞ –Ω–∞ —Å—Ç–∞—Ç—å—é")
    source: Optional[str] = Field("Unknown", description="–ò—Å—Ç–æ—á–Ω–∏–∫")
    published: Optional[str] = Field(None, description="–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏")
    author: Optional[str] = Field(None, description="–ê–≤—Ç–æ—Ä —Å—Ç–∞—Ç—å–∏")

class ProcessRequest(BaseModel):
    articles: List[ArticleCreate] = Field(..., description="–°–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
    eps: float = Field(0.4, ge=0.1, le=1.0, description="–ü–∞—Ä–∞–º–µ—Ç—Ä –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ DBSCAN")
    min_samples: int = Field(1, ge=1, le=10, description="–ú–∏–Ω–∏–º—É–º —Å—Ç–∞—Ç–µ–π –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ")

class RSSProcessRequest(BaseModel):
    eps: float = Field(0.4, ge=0.1, le=1.0, description="–ü–∞—Ä–∞–º–µ—Ç—Ä –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ DBSCAN")
    min_samples: int = Field(1, ge=1, le=10, description="–ú–∏–Ω–∏–º—É–º —Å—Ç–∞—Ç–µ–π –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ")
    json_path: Optional[str] = Field(None, description="–ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: /app/rss_parser/output/rss_feed_4h.json)")

class ArticleResponse(BaseModel):
    id: uuid.UUID
    title: str
    summary: Optional[str]
    short_summary: Optional[str]
    link: str
    source: Optional[str]
    published: Optional[str]
    author: Optional[str]
    hashtags: List[str]
    entities: Dict[str, List[str]]
    cluster_id: Optional[uuid.UUID]
    related_articles: List[Dict[str, str]]
    
    class Config:
        from_attributes = True

class ClusterResponse(BaseModel):
    id: uuid.UUID
    cluster_number: int
    name: str
    summary: str
    articles_count: int
    
    class Config:
        from_attributes = True

class RecommendationResponse(BaseModel):
    article: ArticleResponse
    score: float
    reason: str

# ==================== UTILITY FUNCTIONS ====================

def normalize_hashtag(tag: str) -> Optional[str]:
    tag = tag.lstrip('#')
    
    # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã, —Ü–∏—Ñ—Ä—ã –∏ _
    clean_tag = re.sub(r'[^\w]', '', tag, flags=re.UNICODE)
    
    if not clean_tag:
        return None
    
    return f"#{clean_tag.lower()}"


def normalize_hashtags(hashtags: List[str]) -> List[str]:
    normalized = []
    for tag in hashtags:
        normalized_tag = normalize_hashtag(tag)
        if normalized_tag and normalized_tag not in normalized:
            normalized.append(normalized_tag)
    return normalized


def hash_password(password: str) -> str:
    return hashlib.sha256(password.encode()).hexdigest()


def verify_password(password: str, hashed: str) -> bool:
    return hash_password(password) == hashed


def create_access_token(data: dict, expires_delta: Optional[timedelta] = None):
    to_encode = data.copy()
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    return encoded_jwt


def decode_token(token: str) -> dict:
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        return payload
    except jwt.PyJWTError:
        return None


def load_rss_json(file_path: str = None) -> Dict[str, Any]:
    if file_path is None:
        file_path = RSS_JSON_PATH
    else:
        file_path = Path(file_path)
    
    if not file_path.exists():
        raise HTTPException(
            status_code=404,
            detail=f"RSS JSON —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}"
        )
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data
    except json.JSONDecodeError as e:
        raise HTTPException(
            status_code=400,
            detail=f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {str(e)}"
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {str(e)}"
        )


def convert_rss_entries_to_articles(entries: List[Dict]) -> List[Dict]:
    articles = []
    
    for entry in entries:

        source = None

        if entry.get("source_title"):
            source = entry["source_title"]

        elif entry.get("source"):
            if isinstance(entry["source"], dict):
                source = entry["source"].get("title") or entry["source"].get("href")
            elif isinstance(entry["source"], str):
                source = entry["source"]

        elif entry.get("link"):
            try:
                from urllib.parse import urlparse
                parsed = urlparse(entry["link"])
                domain = parsed.netloc
                # –£–±–∏—Ä–∞–µ–º www. –µ—Å–ª–∏ –µ—Å—Ç—å
                if domain.startswith("www."):
                    domain = domain[4:]
                source = domain
            except Exception as e:
                logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∏—Å—Ç–æ—á–Ω–∏–∫ –∏–∑ —Å—Å—ã–ª–∫–∏: {e}")

        if not source and entry.get("feed_title"):
            source = entry["feed_title"]

        if not source:
            source = "Unknown"
        
        article = {
            "title": entry.get("title", "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"),
            "summary": entry.get("full_text") or entry.get("summary"),
            "link": entry.get("link", ""),
            "source": source,
            "published": entry.get("published", ""),
            "author": entry.get("author") if entry.get("author") else None
        }
        
        if article["link"]:
            articles.append(article)
            logger.debug(f"–°—Ç–∞—Ç—å—è –¥–æ–±–∞–≤–ª–µ–Ω–∞: {article['title'][:50]}... | –ò—Å—Ç–æ—á–Ω–∏–∫: {source}")
    
    logger.info(f"–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π –∏–∑ {len(entries)} –∑–∞–ø–∏—Å–µ–π")
    return articles

# ==================== FASTAPI APP ====================

app = FastAPI(
    title="News Processing & Recommendation API",
    description="API –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π —Å PostgreSQL –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π RSS",
    version="2.0.0" 
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

security = HTTPBearer()

cluster_analyzer = None 

async def generate_cluster_summary(articles_summaries: List[str]) -> Dict[str, Any]:
    if not cluster_analyzer:
        logger.error("Cluster analyzer –Ω–µ –±—ã–ª –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.")
        return {
            "title": f"–ö–ª–∞—Å—Ç–µ—Ä –∏–∑ {len(articles_summaries)} –Ω–æ–≤–æ—Å—Ç–µ–π",
            "summary": "–°–µ—Ä–≤–∏—Å –∞–Ω–∞–ª–∏–∑–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω."
        }
    
    try:
        logger.debug(f"–û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤ LLM –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ {len(articles_summaries)} —Å—Ç–∞—Ç–µ–π...")
        analysis = await asyncio.wait_for(
            cluster_analyzer.analyze_cluster(articles_summaries),
            timeout=None  # –¢–∞–π–º–∞—É—Ç 300 —Å–µ–∫—É–Ω–¥
        )
        
        if not analysis or not hasattr(analysis, 'title') or not hasattr(analysis, 'summary'):
            logger.error(f"LLM –≤–µ—Ä–Ω—É–ª –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –æ–±—ä–µ–∫—Ç: {analysis}")
            return {
                "title": f"–ö–ª–∞—Å—Ç–µ—Ä –∏–∑ {len(articles_summaries)} –Ω–æ–≤–æ—Å—Ç–µ–π",
                "summary": "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ–∑—é–º–µ."
            }
        
        logger.info(f"‚úÖ LLM —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª –∑–∞–≥–æ–ª–æ–≤–æ–∫: '{analysis.title}'")
        return {
            "title": analysis.title,
            "summary": analysis.summary
        }
        
    except asyncio.TimeoutError:
        logger.error(f"‚è±Ô∏è –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∞–º–º–∞—Ä–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞ (>30 —Å–µ–∫)")
        return {
            "title": f"–ö–ª–∞—Å—Ç–µ—Ä –∏–∑ {len(articles_summaries)} –Ω–æ–≤–æ—Å—Ç–µ–π",
            "summary": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ–∑—é–º–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ –∏–∑-–∑–∞ —Ç–∞–π–º–∞—É—Ç–∞."
        }
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∞–º–º–∞—Ä–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞: {type(e).__name__}: {e}", exc_info=True)
        return {
            "title": f"–ö–ª–∞—Å—Ç–µ—Ä –∏–∑ {len(articles_summaries)} –Ω–æ–≤–æ—Å—Ç–µ–π",
            "summary": f"–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ–∑—é–º–µ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ. –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã –º–æ–∂–Ω–æ –ø–æ–Ω—è—Ç—å –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º —Å—Ç–∞—Ç–µ–π."
        }

async def process_rss_on_startup():
    return #–í—ã–∫–ª—é—á–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏
    logger.info("–ó–∞–ø—É—Å–∫ —Ñ–æ–Ω–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ RSS –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ...")
    db = SessionLocal()
    try:
        rss_data = load_rss_json()
        entries = rss_data.get("entries", [])
        if not entries:
            logger.info("–í RSS —Ñ–∞–π–ª–µ –Ω–µ—Ç —Å—Ç–∞—Ç–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
            return

        articles_dict = convert_rss_entries_to_articles(entries)
        
        try:
            from service_code import UnifiedNewsService
        except ImportError:
            logger.error("–ú–æ–¥—É–ª—å 'service_code' –Ω–µ –Ω–∞–π–¥–µ–Ω. –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ —Å—Ç–∞—Ä—Ç–µ –æ—Ç–º–µ–Ω–µ–Ω–∞.")
            return

        service = UnifiedNewsService()
        result = service.process_articles(articles_dict, eps=0.4, min_samples=1)
        
        cluster_summaries_tasks = []
        cluster_data_list = []
        for cluster_data in result['clusters']:
            articles_summaries = [a.get('summary', a['title']) for a in cluster_data['articles']]
            cluster_data_list.append(cluster_data)
            cluster_summaries_tasks.append(generate_cluster_summary(articles_summaries))
        
        logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ LLM-–∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –¥–ª—è {len(cluster_summaries_tasks)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...")
        cluster_summaries = await asyncio.gather(*cluster_summaries_tasks, return_exceptions=True)
        logger.info(f"‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è LLM-–∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {len(cluster_summaries)}")

        created_clusters = {}
        created_articles = {}

        # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò LLM –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
        for idx, cluster_data in enumerate(cluster_data_list):
            llm_result = cluster_summaries[idx]
            cluster_id = cluster_data['cluster_id']
            
            # –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ LLM (–∫–∞–∫ –≤ _process_and_save_articles)
            if isinstance(llm_result, Exception):
                logger.error(f"‚ùå –ö–ª–∞—Å—Ç–µ—Ä {cluster_id}: LLM –∑–∞–¥–∞—á–∞ –ø—Ä–æ–≤–∞–ª–∏–ª–∞—Å—å - {type(llm_result).__name__}: {llm_result}")
                cluster_name = f"–ö–ª–∞—Å—Ç–µ—Ä #{cluster_id}"
                cluster_summary_text = '–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ–∑—é–º–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.'
            elif not isinstance(llm_result, dict):
                logger.error(f"‚ùå –ö–ª–∞—Å—Ç–µ—Ä {cluster_id}: –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ç–∏–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ - {type(llm_result)}, –∑–Ω–∞—á–µ–Ω–∏–µ: {llm_result}")
                cluster_name = f"–ö–ª–∞—Å—Ç–µ—Ä #{cluster_id}"
                cluster_summary_text = '–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ–∑—é–º–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ.'
            elif 'title' not in llm_result or 'summary' not in llm_result:
                logger.error(f"‚ùå –ö–ª–∞—Å—Ç–µ—Ä {cluster_id}: –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–ª—é—á–∏ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ. –ö–ª—é—á–∏: {llm_result.keys()}")
                cluster_name = f"–ö–ª–∞—Å—Ç–µ—Ä #{cluster_id}"
                cluster_summary_text = llm_result.get('summary', '–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ–∑—é–º–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ.')
            else:
                cluster_name = llm_result['title']
                cluster_summary_text = llm_result['summary']
                logger.info(f"‚úÖ –ö–ª–∞—Å—Ç–µ—Ä {cluster_id}: –ó–∞–≥–æ–ª–æ–≤–æ–∫ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω - '{cluster_name[:50]}...'")
            
            cluster = Cluster(
                cluster_number=int(cluster_id),
                name=cluster_name,
                summary=cluster_summary_text
            )
            db.add(cluster)
            db.flush()
            
            logger.info(f"üíæ –ö–ª–∞—Å—Ç–µ—Ä {cluster_id} —Å–æ–∑–¥–∞–Ω —Å ID={cluster.id}, name='{cluster.name[:50]}...'")
            created_clusters[cluster_data['cluster_id']] = cluster

        # –õ–æ–≥–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç–µ–π (–æ—Å—Ç–∞–µ—Ç—Å—è –ø—Ä–µ–∂–Ω–µ–π)
        logger.info(f"–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç–∞—Ç–µ–π...")
        for cluster_data in result['clusters']:
            cluster = created_clusters[cluster_data['cluster_id']]
            for article_data in cluster_data['articles']:
                normalized_tags = normalize_hashtags(article_data.get('hashtags', []))
                existing_article = db.query(Article).filter(Article.link == article_data['link']).first()
                if existing_article:
                    article = existing_article
                    # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ–ª—è, –µ—Å–ª–∏ —Å—Ç–∞—Ç—å—è —É–∂–µ –µ—Å—Ç—å
                    article.title = article_data['title']
                    article.summary = article_data.get('summary')
                    article.short_summary = article_data.get('short_summary')
                    article.hashtags = normalized_tags
                    article.entities = article_data.get('entities', {})
                    article.author = article_data.get('author')
                    article.cluster_id = cluster.id
                    logger.debug(f"–û–±–Ω–æ–≤–ª–µ–Ω–∞ —Å—Ç–∞—Ç—å—è: {article.title[:30]}...")
                else:
                    article = Article(
                        link=article_data['link'],
                        title=article_data['title'],
                        summary=article_data.get('summary'),
                        short_summary=article_data.get('short_summary'),
                        source=article_data.get('source'),
                        published=article_data.get('published'),
                        author=article_data.get('author'),
                        hashtags=normalized_tags,
                        entities=article_data.get('entities', {}),
                        cluster_id=cluster.id
                    )
                    db.add(article)
                    logger.debug(f"–°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è —Å—Ç–∞—Ç—å—è: {article.title[:30]}...")
                db.flush()
                created_articles[article_data['link']] = article
        
        logger.info(f"–°–≤—è–∑—ã–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–µ–π...")
        for cluster_data in result['clusters']:
            for article_data in cluster_data['articles']:
                article = created_articles[article_data['link']]
                related = article_data.get('related_articles_in_cluster', [])
                if related:
                    for rel_data in related:
                        related_article = created_articles.get(rel_data['link'])
                        if related_article and related_article not in article.related_articles:
                            article.related_articles.append(related_article)
        
        logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –ë–î...")
        db.commit()
        
        successful_summaries = sum(
            1 for s in cluster_summaries 
            if isinstance(s, dict) and 'title' in s and 'summary' in s
        )
        
        logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ RSS –Ω–∞ —Å—Ç–∞—Ä—Ç–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
        logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        logger.info(f"   - –°–æ–∑–¥–∞–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {len(created_clusters)}")
        logger.info(f"   - –°–æ–∑–¥–∞–Ω–æ/–æ–±–Ω–æ–≤–ª–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(created_articles)}")
        logger.info(f"   - LLM-–∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: {successful_summaries}/{len(cluster_summaries)}")
        logger.info(f"   - LLM-–∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –ø—Ä–æ–≤–∞–ª–∏–ª–æ—Å—å: {len(cluster_summaries) - successful_summaries}")

    except HTTPException as e:
        logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ RSS –Ω–∞ —Å—Ç–∞—Ä—Ç–µ: {e.detail}")
        db.rollback()
    except Exception as e:
        logger.critical(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ RSS –Ω–∞ —Å—Ç–∞—Ä—Ç–µ: {e}", exc_info=True)
        db.rollback()
    finally:
        db.close()

@app.on_event("startup")
async def startup_event():
    global cluster_analyzer
    logger.info("–ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è...")
    cluster_analyzer = TextClusterAnalyzer(
        api_key=os.getenv("OPENAI_API_KEY", "not-needed"),
        base_url=os.getenv("SGLANG_URL", "http://sglang:30000/v1"),
        model=os.getenv("OPENAI_MODEL", "Qwen/Qwen3-4B-AWQ")
    )
    asyncio.create_task(process_rss_on_startup())
    logger.info("–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∑–∞–ø—É—â–µ–Ω–æ. –û–±—Ä–∞–±–æ—Ç–∫–∞ RSS –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ.")

@app.on_event("shutdown")
async def shutdown_event():
    global cluster_analyzer
    if cluster_analyzer:
        await cluster_analyzer.close()
    logger.info("–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ.")

# ==================== DEPENDENCY ====================

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


async def get_current_user(credentials: HTTPAuthorizationCredentials = Depends(security), db: Session = Depends(get_db)) -> User:
    token = credentials.credentials
    payload = decode_token(token)
    if not payload or not (user_id := payload.get("sub")):
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Invalid authentication credentials")
    
    user = db.query(User).filter(User.id == uuid.UUID(user_id)).first()
    if user is None:
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="User not found")
    
    return user

# ==================== AUTH ENDPOINTS ====================

@app.post("/api/v1/auth/register", response_model=Token, tags=["Authentication"])
async def register(user_data: UserCreate, db: Session = Depends(get_db)):
    existing_user = db.query(User).filter(User.login == user_data.login).first()
    if existing_user:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Login already registered"
        )
    
    user = User(
        login=user_data.login,
        password_hash=hash_password(user_data.password)
    )
    
    db.add(user)
    db.commit()
    db.refresh(user)
    
    access_token = create_access_token(
        data={"sub": str(user.id)},
        expires_delta=timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    )
    
    return {"access_token": access_token, "token_type": "bearer"}


@app.post("/api/v1/auth/login", response_model=Token, tags=["Authentication"])
async def login(user_data: UserLogin, db: Session = Depends(get_db)):
    user = db.query(User).filter(User.login == user_data.login).first()
    
    if not user or not verify_password(user_data.password, user.password_hash):
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect login or password"
        )
    
    access_token = create_access_token(
        data={"sub": str(user.id)},
        expires_delta=timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    )
    
    return {"access_token": access_token, "token_type": "bearer"}

# ==================== ARTICLES ENDPOINTS ====================
async def _process_and_save_articles(articles_dict: List[Dict], eps: float, min_samples: int, db: Session):
    try:
        from service_code import UnifiedNewsService
    except ImportError:
         raise HTTPException(status_code=500, detail="–ú–æ–¥—É–ª—å 'service_code' –Ω–µ –Ω–∞–π–¥–µ–Ω.")

    service = UnifiedNewsService()
    result = service.process_articles(articles_dict, eps=eps, min_samples=min_samples)

    cluster_summaries_tasks = []
    cluster_data_list = []
    for cluster_data in result['clusters']:
        articles_summaries = [a.get('summary', a['title']) for a in cluster_data['articles']]
        cluster_data_list.append(cluster_data)
        cluster_summaries_tasks.append(generate_cluster_summary(articles_summaries))

    logger.info(f"–ó–∞–ø—É—Å–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –¥–ª—è {len(cluster_summaries_tasks)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...")
    cluster_summaries = await asyncio.gather(*cluster_summaries_tasks, return_exceptions=True)
    logger.info(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {len(cluster_summaries)}")

    created_clusters = {}
    created_articles = {}
    
    for idx, cluster_data in enumerate(cluster_data_list):
        llm_result = cluster_summaries[idx]
        cluster_id = cluster_data['cluster_id']
        
        if isinstance(llm_result, Exception):
            logger.error(f"‚ùå –ö–ª–∞—Å—Ç–µ—Ä {cluster_id}: LLM –∑–∞–¥–∞—á–∞ –ø—Ä–æ–≤–∞–ª–∏–ª–∞—Å—å - {type(llm_result).__name__}: {llm_result}")
            cluster_name = f"–ö–ª–∞—Å—Ç–µ—Ä #{cluster_id}"
            cluster_summary_text = '–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ–∑—é–º–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.'
        elif not isinstance(llm_result, dict):
            logger.error(f"‚ùå –ö–ª–∞—Å—Ç–µ—Ä {cluster_id}: –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ç–∏–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ - {type(llm_result)}, –∑–Ω–∞—á–µ–Ω–∏–µ: {llm_result}")
            cluster_name = f"–ö–ª–∞—Å—Ç–µ—Ä #{cluster_id}"
            cluster_summary_text = '–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ–∑—é–º–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ.'
        elif 'title' not in llm_result or 'summary' not in llm_result:
            logger.error(f"‚ùå –ö–ª–∞—Å—Ç–µ—Ä {cluster_id}: –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–ª—é—á–∏ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ. –ö–ª—é—á–∏: {llm_result.keys()}")
            cluster_name = f"–ö–ª–∞—Å—Ç–µ—Ä #{cluster_id}"
            cluster_summary_text = llm_result.get('summary', '–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ–∑—é–º–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ.')
        else:
            cluster_name = llm_result['title']
            cluster_summary_text = llm_result['summary']
            logger.info(f"‚úÖ –ö–ª–∞—Å—Ç–µ—Ä {cluster_id}: –ó–∞–≥–æ–ª–æ–≤–æ–∫ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω - '{cluster_name[:50]}...'")
        
        cluster = Cluster(
            cluster_number=int(cluster_id),
            name=cluster_name,
            summary=cluster_summary_text
        )
        db.add(cluster)
        db.flush()
        
        logger.info(f"üíæ –ö–ª–∞—Å—Ç–µ—Ä {cluster_id} —Å–æ–∑–¥–∞–Ω —Å ID={cluster.id}, name='{cluster.name[:50]}...'")
        created_clusters[cluster_id] = cluster

    logger.info(f"–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç–∞—Ç–µ–π...")
    for cluster_data in result['clusters']:
        cluster = created_clusters[cluster_data['cluster_id']]
        for article_data in cluster_data['articles']:
            normalized_tags = normalize_hashtags(article_data.get('hashtags', []))
            existing_article = db.query(Article).filter(Article.link == article_data['link']).first()
            
            if existing_article:
                article = existing_article
                article.title = article_data['title']
                article.summary = article_data.get('summary')
                article.short_summary = article_data.get('short_summary')
                article.hashtags = normalized_tags
                article.entities = article_data.get('entities', {})
                article.author = article_data.get('author')
                article.cluster_id = cluster.id
                logger.debug(f"–û–±–Ω–æ–≤–ª–µ–Ω–∞ —Å—Ç–∞—Ç—å—è: {article.title[:30]}...")
            else:
                article = Article(
                    link=article_data['link'],
                    title=article_data['title'],
                    summary=article_data.get('summary'),
                    short_summary=article_data.get('short_summary'),
                    source=article_data.get('source'),
                    published=article_data.get('published'),
                    author=article_data.get('author'),
                    hashtags=normalized_tags,
                    entities=article_data.get('entities', {}),
                    cluster_id=cluster.id
                )
                db.add(article)
                logger.debug(f"–°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è —Å—Ç–∞—Ç—å—è: {article.title[:30]}...")
            
            db.flush()
            created_articles[article_data['link']] = article

    logger.info(f"–°–≤—è–∑—ã–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–µ–π...")
    for cluster_data in result['clusters']:
        for article_data in cluster_data['articles']:
            article = created_articles[article_data['link']]
            related = article_data.get('related_articles_in_cluster', [])
            if related:
                for rel_data in related:
                    related_article = created_articles.get(rel_data['link'])
                    if related_article and related_article not in article.related_articles:
                        article.related_articles.append(related_article)
    
    logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –ë–î...")
    db.commit()
    logger.info(f"‚úÖ –í—Å–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    successful_summaries = sum(
        1 for s in cluster_summaries 
        if isinstance(s, dict) and 'title' in s and 'summary' in s
    )
    
    return {
        "processed_articles": result['total_articles'],
        "total_clusters": result['total_clusters'],
        "clusters_created": len(created_clusters),
        "articles_created_or_updated": len(created_articles),
        "llm_summaries_generated": successful_summaries,
        "llm_summaries_failed": len(cluster_summaries) - successful_summaries
    }

@app.post("/api/v1/articles/process-rss", tags=["Articles"])
async def process_rss_articles(request: RSSProcessRequest = Body(...), current_user: User = Depends(get_current_user), db: Session = Depends(get_db)):
    try:
        rss_data = load_rss_json(request.json_path)
        entries = rss_data.get("entries", [])
        if not entries:
            raise HTTPException(status_code=400, detail="–í RSS JSON —Ñ–∞–π–ª–µ –Ω–µ—Ç —Å—Ç–∞—Ç–µ–π")
        
        articles_dict = convert_rss_entries_to_articles(entries)
        if not articles_dict:
            raise HTTPException(status_code=400, detail="–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Å—Ç–∞—Ç—å–∏ –∏–∑ JSON")

        stats = await _process_and_save_articles(articles_dict, request.eps, request.min_samples, db)

        return {
            "status": "success",
            "message": "RSS —Å—Ç–∞—Ç—å–∏ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã —Å LLM-—Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–µ–π",
            "statistics": {
                "total_entries_in_file": rss_data.get("total_entries", 0),
                "time_range_in_file": rss_data.get("time_range", "unknown"),
                **stats
            }
        }
    except Exception as e:
        db.rollback()
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ RSS: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}")


@app.post("/api/v1/articles/process", tags=["Articles"])
async def process_articles(request: ProcessRequest, current_user: User = Depends(get_current_user), db: Session = Depends(get_db)):
    try:
        articles_dict = [article.dict() for article in request.articles]
        stats = await _process_and_save_articles(articles_dict, request.eps, request.min_samples, db)
        
        return {
            "status": "success",
            "message": "–°—Ç–∞—Ç—å–∏ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã —Å LLM-—Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–µ–π",
            "statistics": stats
        }
    except Exception as e:
        db.rollback()
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç–∞—Ç–µ–π: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}")

@app.get("/api/v1/articles", response_model=List[ArticleResponse], tags=["Articles"])
async def get_articles(
    skip: int = Query(0, ge=0),
    limit: int = Query(50, ge=1, le=100),
    cluster_id: Optional[uuid.UUID] = Query(None, description="–§–∏–ª—å—Ç—Ä –ø–æ ID –∫–ª–∞—Å—Ç–µ—Ä–∞"),
    hashtags: Optional[List[str]] = Query(None, description="–°–ø–∏—Å–æ–∫ —Ö—ç—à—Ç–µ–≥–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (–ª–æ–≥–∏–∫–∞ –ò). –ü—Ä–∏–º–µ—Ä: ?hashtags=–ø–æ–ª–∏—Ç–∏–∫–∞&hashtags=–≤—ã–±–æ—Ä—ã"),
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    query = db.query(Article)
    
    if cluster_id:
        query = query.filter(Article.cluster_id == cluster_id)

    if hashtags:
        normalized_tags = normalize_hashtags(hashtags)
        if normalized_tags:
            query = query.filter(Article.hashtags.contains(normalized_tags))
        
    articles = query.offset(skip).limit(limit).all()
    
    result = []
    for article in articles:
        related = [{"title": a.title, "link": a.link} for a in article.related_articles]
        result.append({
            "id": article.id,
            "title": article.title,
            "summary": article.summary,
            "short_summary": article.short_summary,
            "link": article.link,
            "source": article.source,
            "published": article.published,
            "author": article.author,
            "hashtags": article.hashtags or [],
            "entities": article.entities or {},
            "cluster_id": article.cluster_id,
            "related_articles": related
        })
    print(f"articles: {len(result)}")
    return result


@app.get("/api/v1/articles/{article_id}", response_model=ArticleResponse, tags=["Articles"])
async def get_article(
    article_id: uuid.UUID,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    article = db.query(Article).filter(Article.id == article_id).first()
    
    if not article:
        raise HTTPException(status_code=404, detail="–°—Ç–∞—Ç—å—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
    
    related = [{"title": a.title, "link": a.link} for a in article.related_articles]
    
    return {
        "id": article.id,
        "title": article.title,
        "summary": article.summary,
        "short_summary": article.short_summary,
        "link": article.link,
        "source": article.source,
        "published": article.published,
        "author": article.author,
        "hashtags": article.hashtags or [],
        "entities": article.entities or {},
        "cluster_id": article.cluster_id,
        "related_articles": related
    }


@app.post("/api/v1/articles/{article_id}/read", tags=["User Actions"])
async def mark_article_read(
    article_id: uuid.UUID,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    article = db.query(Article).filter(Article.id == article_id).first()
    
    if not article:
        raise HTTPException(status_code=404, detail="–°—Ç–∞—Ç—å—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
    
    if article in current_user.read_articles:
        return {"status": "info", "message": "–°—Ç–∞—Ç—å—è —É–∂–µ –æ—Ç–º–µ—á–µ–Ω–∞ –∫–∞–∫ –ø—Ä–æ—á–∏—Ç–∞–Ω–Ω–∞—è"}
    
    current_user.read_articles.append(article)
    
    topics_dict = current_user.interested_hashtags or {}
    
    for tag in article.hashtags or []:
        topics_dict[tag] = topics_dict.get(tag, 0) + 1
    
    for entity_list in (article.entities or {}).values():
        for entity in entity_list:
            normalized_entity = normalize_hashtag(entity)
            if normalized_entity:
                topics_dict[normalized_entity] = topics_dict.get(normalized_entity, 0) + 1
            
    current_user.interested_hashtags = topics_dict
    
    if article.author:
        authors_dict = current_user.interested_authors or {}
        authors_dict[article.author] = authors_dict.get(article.author, 0) + 1
        current_user.interested_authors = authors_dict
    
    if article.cluster_id:
        clusters_dict = current_user.interested_clusters or {}
        cluster_key = str(article.cluster_id)
        clusters_dict[cluster_key] = clusters_dict.get(cluster_key, 0) + 1
        current_user.interested_clusters = clusters_dict
    
    db.commit()
    
    return {
        "status": "success",
        "message": "–°—Ç–∞—Ç—å—è –æ—Ç–º–µ—á–µ–Ω–∞ –∫–∞–∫ –ø—Ä–æ—á–∏—Ç–∞–Ω–Ω–∞—è",
        "article_title": article.title
    }

# ==================== CLUSTERS ENDPOINTS ====================

@app.get("/api/v1/clusters", response_model=List[ClusterResponse], tags=["Clusters"])
async def get_clusters(
    skip: int = Query(0, ge=0),
    limit: int = Query(50, ge=1, le=100),
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    clusters = db.query(Cluster).offset(skip).limit(limit).all()
    
    result = []
    for cluster in clusters:
        result.append({
            "id": cluster.id,
            "cluster_number": cluster.cluster_number,
            "name": cluster.name,
            "summary": cluster.summary,
            "articles_count": len(cluster.articles)
        })
    print(f"clusters: {len(result)}")
    return result


@app.get("/api/v1/clusters/{cluster_id}", tags=["Clusters"])
async def get_cluster(
    cluster_id: uuid.UUID,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    cluster = db.query(Cluster).filter(Cluster.id == cluster_id).first()
    
    if not cluster:
        raise HTTPException(status_code=404, detail="–ö–ª–∞—Å—Ç–µ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    articles = []
    for article in cluster.articles:
        related = [{"title": a.title, "link": a.link} for a in article.related_articles]
        articles.append({
            "id": article.id,
            "title": article.title,
            "summary": article.summary,
            "short_summary": article.short_summary,
            "link": article.link,
            "author": article.author,
            "hashtags": article.hashtags or [],
            "related_articles": related
        })
    
    return {
        "id": cluster.id,
        "cluster_number": cluster.cluster_number,
        "name": cluster.name,
        "summary": cluster.summary,
        "articles_count": len(articles),
        "articles": articles
    }

# ==================== HASHTAGS ENDPOINTS ====================

@app.get("/api/v1/hashtags", response_model=List[str], tags=["Hashtags"])
async def get_hashtags(
    search: Optional[str] = Query(None, description="–ü–æ–∏—Å–∫ –ø–æ —á–∞—Å—Ç–∏ —Ö—ç—à—Ç–µ–≥–∞ (–±–µ–∑ —É—á–µ—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞)"),
    limit: int = Query(100, ge=1, le=500, description="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ö—ç—à—Ç–µ–≥–æ–≤ –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞"),
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ö—ç—à—Ç–µ–≥–æ–≤ –∏–∑ –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π.
    –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –∞–≤—Ç–æ–¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –Ω–∞ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–µ.
    """
    query = db.query(func.unnest(Article.hashtags).label("hashtag")).distinct()
    
    if search:
        normalized_search = normalize_hashtag(search)
        if normalized_search:
            query = query.filter(func.unnest(Article.hashtags).ilike(f"%{normalized_search}%"))

    results = query.limit(limit).all()
    
    return [row.hashtag for row in results]


# ==================== RECOMMENDATIONS ====================

@app.get("/api/v1/recommendations", response_model=Dict[str, Any], tags=["Recommendations"])
async def get_recommendations(
    top_n: int = Query(10, ge=1, le=50),
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    read_ids = [a.id for a in current_user.read_articles]
    articles = db.query(Article).filter(~Article.id.in_(read_ids)).all()
    
    if not articles:
        return {
            "recommendations": [],
            "message": "–ù–µ—Ç –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"
        }
    
    scored_articles = []
    
    for article in articles:
        score = 0.0
        reasons = []
        
        user_topics = current_user.interested_hashtags or {}
        topic_score = 0.0
        common_topics = []
        
        article_topics = set(article.hashtags or [])
        for entity_list in (article.entities or {}).values():
            for entity in entity_list:
                normalized_entity = normalize_hashtag(entity)
                if normalized_entity:
                    article_topics.add(normalized_entity)

        for topic in article_topics:
            if topic in user_topics:
                topic_score += user_topics[topic]
                common_topics.append(topic)
        
        if common_topics:
            reasons.append(f"–¢–µ–º—ã: {', '.join(common_topics[:2])}")
        
        score += 0.55 * topic_score 
        
        user_authors = current_user.interested_authors or {}
        if article.author and article.author in user_authors:
            author_score = user_authors[article.author]
            score += 0.45 * author_score
            reasons.append(f"–ê–≤—Ç–æ—Ä: {article.author}")
        
        if article.cluster_id:
            user_clusters = current_user.interested_clusters or {}
            cluster_key = str(article.cluster_id)
            if cluster_key in user_clusters:
                score += 0.15 * user_clusters[cluster_key]
                reasons.append("–ü–æ—Ö–æ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏")
        
        if not reasons:
            reasons.append("–ù–æ–≤–∞—è —Ç–µ–º–∞ –¥–ª—è –≤–∞—Å")
        
        scored_articles.append({
            "article": article,
            "score": score,
            "reason": " | ".join(reasons)
        })
    
    scored_articles.sort(key=lambda x: x['score'], reverse=True)
    
    recommendations = []
    for item in scored_articles[:top_n]:
        article = item['article']
        related = [{"title": a.title, "link": a.link} for a in article.related_articles]
        
        recommendations.append({
            "article": {
                "id": article.id,
                "title": article.title,
                "summary": article.summary,
                "short_summary": article.short_summary,
                "link": article.link,
                "source": article.source,
                "published": article.published,
                "author": article.author,
                "hashtags": article.hashtags or [],
                "entities": article.entities or {},
                "cluster_id": article.cluster_id,
                "related_articles": related
            },
            "score": round(item['score'], 2),
            "reason": item['reason']
        })
    print(f"recommendations: {len(recommendations)}")
    return {"recommendations": recommendations}

# ==================== USER PROFILE ====================

@app.get("/api/v1/profile", tags=["User Profile"])
async def get_profile(
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    hashtags = current_user.interested_hashtags or {}
    authors = current_user.interested_authors or {}
    clusters = current_user.interested_clusters or {}
    
    top_hashtags = dict(sorted(hashtags.items(), key=lambda x: x[1], reverse=True)[:10])
    top_authors = dict(sorted(authors.items(), key=lambda x: x[1], reverse=True)[:10])
    
    return {
        "user_id": str(current_user.id),
        "login": current_user.login,
        "statistics": {
            "read_articles_count": len(current_user.read_articles),
            "unique_topics_count": len(hashtags),
            "unique_authors": len(authors),
            "preferred_clusters": len(clusters)
        },
        "interests": {
            "top_topics": top_hashtags,
            "top_authors": top_authors
        },
        "recent_articles": [
            {"id": str(a.id), "title": a.title, "link": a.link}
            for a in current_user.read_articles[-20:]
        ]
    }

# ==================== RSS INFO ====================

@app.get("/api/v1/rss/info", tags=["RSS"])
async def get_rss_info(
    json_path: Optional[str] = Query(None, description="–ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É"),
    current_user: User = Depends(get_current_user)
):
    try:
        rss_data = load_rss_json(json_path)
        entries = rss_data.get("entries", [])
        
        sources = {}
        for entry in entries:
            source = entry.get("source_title", "Unknown")
            sources[source] = sources.get(source, 0) + 1
        
        return {
            "file_info": {
                "generated_at": rss_data.get("generated_at"),
                "total_entries": rss_data.get("total_entries", 0),
                "time_range": rss_data.get("time_range", "unknown")
            },
            "sources": sources,
            "sample_entries": entries[:5]
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {str(e)}")

# ==================== GENERAL ====================

@app.get("/", tags=["General"])
async def root():
    return {
        "message": "News Processing & Recommendation API v2.0",
        "database": "PostgreSQL",
        "features": [
            "Automatic RSS JSON processing",
            "News clustering with DBSCAN",
            "Personalized recommendations",
            "User interest tracking",
            "Normalized hashtags (format: #tag_name)"
        ],
        "endpoints": {
            "docs": "/docs",
            "rss_processing": "/api/v1/articles/process-rss",
            "recommendations": "/api/v1/recommendations"
        }
    }


@app.get("/health", tags=["General"])
async def health_check(db: Session = Depends(get_db)):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞ –¥–ª—è Docker"""
    try:
        db.execute("SELECT 1")
        db_status = "healthy"
    except Exception as e:
        db_status = f"unhealthy: {str(e)}"
    
    rss_file_exists = RSS_JSON_PATH.exists() if RSS_JSON_PATH else False
    
    return {
        "status": "healthy" if db_status == "healthy" else "degraded",
        "database": db_status,
        "rss_file": "found" if rss_file_exists else "not found",
        "rss_file_path": str(RSS_JSON_PATH),
        "timestamp": datetime.utcnow().isoformat()
    }

# ==================== –ó–ê–ü–£–°–ö ====================

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "api:app",
        host="0.0.0.0",
        port=int(os.getenv("PORT", 8000)),
        reload=False,
        log_level=os.getenv("LOG_LEVEL", "info")
    )